options:
  logging: CLOUD_LOGGING_ONLY

steps:
# Upload DAG files to Cloud Composer DAG bucket
- name: 'gcr.io/cloud-builders/gsutil'
  args: ['-m', 'cp', '-r', 'dags/*', 'gs://us-central1-blockpulse-cryp-96d4960a-bucket/dags/']

# Upload ETL scripts to match the ETL_PATH in DAG
- name: 'gcr.io/cloud-builders/gsutil'
  args: ['-m', 'cp', '-r', 'scripts/*', 'gs://blockpulse-data-bucket/etl/']

# Upload BigQuery schema SQL
- name: 'gcr.io/cloud-builders/gsutil'
  args: ['-m', 'cp', '-r', 'sql/*', 'gs://blockpulse-data-bucket/sql/']

# Upload documentation (README, etc.)
- name: 'gcr.io/cloud-builders/gsutil'
  args: ['-m', 'cp', 'README.md', 'docs/README_addendum.md', 'gs://blockpulse-data-bucket/docs/']

# Upload requirements.txt for traceability (optional)
- name: 'gcr.io/cloud-builders/gsutil'
  args: ['cp', 'requirements.txt', 'gs://blockpulse-data-bucket/requirements/']

# Upload cloudbuild.yaml itself for auditing
- name: 'gcr.io/cloud-builders/gsutil'
  args: ['cp', 'cloudbuild.yaml', 'gs://blockpulse-data-bucket/configs/']

# Optional: Success message
- name: 'alpine'
  entrypoint: 'echo'
  args: ['Deployment complete: DAGs, ETL scripts, SQL, and docs uploaded to GCS.']