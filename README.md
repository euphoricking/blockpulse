# ğŸš€ CoinGecko Crypto Data Pipeline (GCP + Apache Beam + BigQuery)

This project implements a fully cloud-native ETL pipeline that fetches cryptocurrency market data from the [CoinGecko API](https://www.coingecko.com/), processes it using **Apache Beam**, stores it in **Google Cloud Storage**, and loads it into a **BigQuery data warehouse** following a **star schema**.

---

## ğŸ“¦ Project Structure

```
coingecko-pipeline/
â”œâ”€â”€ dags/                         # Airflow DAGs
â”‚   â””â”€â”€ crypto_etl_dag.py
â”œâ”€â”€ scripts/                      # Beam-compatible ETL scripts
â”‚   â”œâ”€â”€ fetch_crypto_data.py
â”‚   â”œâ”€â”€ populate_crypto_asset_dim.py
â”‚   â””â”€â”€ populate_date_dim.py
â”œâ”€â”€ sql/                          # BigQuery schema setup
â”‚   â””â”€â”€ create_tables.sql
â”œâ”€â”€ docs/                         # Project artifacts and documentation
â”‚   â”œâ”€â”€ dimensional_model.md
â”‚   â”œâ”€â”€ architecture_diagram_description.md
â”‚   â””â”€â”€ README_addendum.md
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ cloudbuild.yaml               # GCP CI/CD pipeline
â””â”€â”€ README.md                     # Project overview
```

---

## ğŸ“Š Star Schema Overview

**Fact Table**
- `crypto_market_snapshot_fact`: Daily metrics per coin (price, volume, market cap, volatility)

**Dimension Tables**
- `crypto_asset_dim`: Coin metadata (name, symbol, category, launch date)
- `date_dim`: Calendar table for BI reporting

---

## âš™ï¸ Tools & Services Used

| Component       | Technology                |
|----------------|----------------------------|
| API Source      | CoinGecko REST API         |
| Data Processing | Apache Beam (Python)       |
| Orchestration   | Cloud Composer (Airflow)   |
| Data Storage    | Google Cloud Storage       |
| Data Warehouse  | BigQuery                   |
| CI/CD           | Cloud Build + GitHub       |

---

## ğŸ” Pipeline Workflow

1. DAG is triggered daily via Cloud Composer
2. SQL schema is read from GCS and used to initialize BigQuery tables
3. Beam jobs are executed on **Dataflow**:
    - `fetch_crypto_data.py`: Coin metrics â†’ GCS â†’ BigQuery
    - `populate_crypto_asset_dim.py`: Coin metadata â†’ GCS â†’ BigQuery
    - `populate_date_dim.py`: Date records â†’ GCS â†’ BigQuery

---

## ğŸš€ Deployment Steps

1. **Set up GCS Buckets**
   - `your-composer-bucket` (for DAGs)
   - `your-data-bucket` (for SQL, docs, outputs)

2. **Create BigQuery Dataset**
   ```sql
   CREATE SCHEMA `your-project-id.crypto_data`;
   ```

3. **Run Cloud Build via Trigger**
   - Trigger on GitHub push using `cloudbuild.yaml`
   - This will upload:
     - DAGs â†’ `gs://your-composer-bucket/dags/`
     - Scripts â†’ `gs://your-composer-bucket/dags/scripts/`
     - SQL â†’ `gs://your-data-bucket/sql/`
     - Docs and config â†’ appropriate folders

4. **Monitor DAG in Composer UI**
   - View tasks and logs
   - Ensure success on daily runs

---

## ğŸ“ Required Permissions

- Cloud Build service account:
  - `Storage Admin`
  - `Composer Worker`
- Composer environment should have:
  - Apache Beam SDK installed
  - Access to `gsutil` and BigQuery

---

## ğŸ› ï¸ Optimization Tips

- Enable BigQuery table partitioning and clustering
- Monitor Dataflow pipeline cost and auto-scaling
- Use caching for API rate-limited endpoints

---

## ğŸ™Œ Authors

- Developed by [Your Name]
- Inspired by best practices from [10Alytics Capstone](https://10alytics.com)

---

## ğŸ“„ License

This project is licensed under the MIT License. Feel free to adapt and extend for your own use.